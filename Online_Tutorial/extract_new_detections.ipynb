{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script 3:\n",
    "\n",
    "FMF only returns time series of correlation coefficients. In this script, we show how to use these correlation coefficient time series in order to detect new earthquakes, and extract their waveforms. The functions defined in this script can be reused in another template matching earthquake detection framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.getcwd())\n",
    "\n",
    "import h5py as h5\n",
    "import numpy as np\n",
    "import utils\n",
    "\n",
    "from obspy.core import UTCDateTime as udt\n",
    "from time import time as give_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First, define the function we will call to extract the waveforms of the newly detected earthquakes\n",
    "\n",
    "This function expects 7 arguments:\n",
    "- data $(n_{\\mathrm{stations}} \\times n_{\\mathrm{components}} \\times n_{\\mathrm{samples-in-data}})$\n",
    "- cc_sums $(n_{\\mathrm{templates}} \\times n_{\\mathrm{correlations}})$ (*i.e.* the output of FMF)\n",
    "- moveout_array $(n_{\\mathrm{templates}} \\times n_{\\mathrm{stations}} \\times n_{\\mathrm{components}})$\n",
    "- n_mad (determines the detection threshold: $n_{\\mathrm{mad}} \\times \\mathrm{MAD}(\\mathrm{CC}(t))$)\n",
    "- template_duration (in seconds, used for choosing the minimum distance between consecutive detections)\n",
    "- extracted_duration (in seconds, the duration extracted on each station / channel)\n",
    "\n",
    "Without going into details, this function returns Python lists of metadata and of waveforms with length $n_{\\mathrm{templates}}$. Each element of these lists contains a Python dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_new_detections(data,\n",
    "                           cc_sums,\n",
    "                           moveout_array,\n",
    "                           n_mad=10.,\n",
    "                           template_duration=8.,\n",
    "                           step=1,\n",
    "                           extracted_duration=60.):\n",
    "\n",
    "    n_templates = cc_sums.shape[0]\n",
    "    n_stations = moveout_array.shape[1]\n",
    "    n_components = moveout_array.shape[2]\n",
    "    n_extracted_samples = np.int32(extracted_duration * data['metadata']['sampling_rate'])\n",
    "    buffer_extracted_events = 10.\n",
    "\n",
    "    list_metadata = []\n",
    "    list_waveforms = []\n",
    "    for i in range(n_templates):\n",
    "        cc_sum = cc_sums[i, :]\n",
    "\n",
    "        cc_sum -= np.median(cc_sum)\n",
    "        threshold = n_mad * np.median(np.abs(cc_sum))\n",
    "        # ------------------\n",
    "        cc_idx = np.argwhere(cc_sum > threshold)\n",
    "        detections = cc_idx * step\n",
    "\n",
    "        # only keep highest correlation coefficient for grouped detections\n",
    "        # we assume the last component is the vertical component\n",
    "        d_mv = moveout_array[i, :, 0] - moveout_array[i, :, -1]\n",
    "        # fix the maximum window size to 3 times the template duration\n",
    "        # fix the minimum window size to 1 time the templare duration\n",
    "        # in between: choose an adaptive size based on the median\n",
    "        # P-S time\n",
    "        search_win = min(np.int32(3. * template_duration *\n",
    "                                  data['metadata']['sampling_rate'] / step),\n",
    "                         max(np.int32(1. * np.median(d_mv[d_mv != 0]) / step),\n",
    "                             np.int32(template_duration *\n",
    "                                      data['metadata']['sampling_rate'] / step)))\n",
    "        for j in range(cc_idx.size):\n",
    "            idx = np.arange(max(0, cc_idx[j] - search_win // 2),\n",
    "                            min(cc_sum.size-1, cc_idx[j] + search_win // 2),\n",
    "                            dtype=np.int32)\n",
    "            idx_to_update = np.where(cc_idx == cc_idx[j])[0]\n",
    "            cc_idx[idx_to_update] = np.argmax(cc_sum[idx]) + idx[0]\n",
    "\n",
    "        cc_idx = np.unique(cc_idx)\n",
    "        detections = cc_idx * step\n",
    "\n",
    "        # after this step, we can have detections closest than search_win / 2\n",
    "        cc_idx = list(cc_idx)\n",
    "        n_removed = 0\n",
    "        for j in range(1, detections.size):\n",
    "            if (cc_idx[j-n_removed] - cc_idx[j-n_removed-1]) < search_win // 2:\n",
    "                if cc_sum[cc_idx[j-n_removed]] > cc_sum[cc_idx[j-n_removed-1]]:\n",
    "                    cc_idx.remove(cc_idx[j-n_removed-1])\n",
    "                else:\n",
    "                    cc_idx.remove(cc_idx[j-n_removed])\n",
    "                n_removed += 1\n",
    "        cc_idx = np.asarray(cc_idx)\n",
    "        detections = cc_idx * step\n",
    "\n",
    "        n_multiplets = len(detections)\n",
    "        # ------------------------------------------------------\n",
    "        metadata_events = {}\n",
    "        waveforms_events = {}\n",
    "        origin_times = np.zeros(n_multiplets, dtype=np.float64)\n",
    "        correlation_coefficients = np.zeros(n_multiplets, dtype=np.float32)\n",
    "        waveforms = np.zeros((n_multiplets, n_stations,\n",
    "                              n_components, n_extracted_samples), dtype=np.float32)\n",
    "        idx_min = 0  # can't extract continuous data before index 0\n",
    "        idx_max = data['waveforms'].shape[-1]  # can't extract continuous data after\n",
    "        #                                        the last sample of the day\n",
    "        for d in range(n_multiplets):\n",
    "            origin_time = udt(data['metadata']['date']) \\\n",
    "                          + detections[d] / data['metadata']['sampling_rate']\n",
    "            origin_times[d] = origin_time.timestamp \\\n",
    "                - buffer_extracted_events\n",
    "            correlation_coefficients[d] = cc_sum[cc_idx[d]]\n",
    "            # -----------------------------------------\n",
    "            # take care of not selecting out-of-bound indexes:\n",
    "            id1 = detections[d] - np.int32(buffer_extracted_events\n",
    "                                           * data['metadata']['sampling_rate'])\n",
    "            if id1 < idx_min:\n",
    "                # will have to zero-pad the beginning of the extracted sequence\n",
    "                dn_b = idx_min - id1\n",
    "                id2 = np.int32(id1 + n_extracted_samples)\n",
    "                id1 = np.int32(idx_min)\n",
    "            else:\n",
    "                dn_b = 0\n",
    "                id2 = id1 + n_extracted_samples\n",
    "            if id2 > idx_max:\n",
    "                # will have to zero-pad the end of the extracted sequence\n",
    "                dn_e = id2 - idx_max\n",
    "                id2 = np.int32(idx_max)\n",
    "            else:\n",
    "                dn_e = 0\n",
    "            waveforms[d, :, :, :] = np.concatenate((np.zeros((n_stations,\n",
    "                                                              n_components,\n",
    "                                                              dn_b),\n",
    "                                                             dtype=np.float32),\n",
    "                                                    data['waveforms'][:,\n",
    "                                                                      :,\n",
    "                                                                      id1:id2],\n",
    "                                                    np.zeros((n_stations,\n",
    "                                                              n_components,\n",
    "                                                              dn_e),\n",
    "                                                             dtype=np.float32)),\n",
    "                                                   axis=-1)\n",
    "            # -----------------------------------------\n",
    "        metadata_events.update({'template_id'                :   np.array([i])})\n",
    "        metadata_events.update({'stations'                   :   np.asarray(data['metadata']['stations']).astype('S')})\n",
    "        metadata_events.update({'components'                 :   np.asarray(data['metadata']['components']).astype('S')})\n",
    "        metadata_events.update({'origin_times'               :   origin_times})\n",
    "        metadata_events.update({'correlation_coefficients'   :   correlation_coefficients})\n",
    "        waveforms_events.update({'waveforms'                 :   waveforms})\n",
    "\n",
    "        list_metadata.append(metadata_events)\n",
    "        list_waveforms.append(waveforms_events)\n",
    "    return list_metadata, list_waveforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Then, define a function to store the extracted events in a database\n",
    "\n",
    "This function creates an h5 file, containing as many groups as there are templates. Each group contains datasets with metadata and waveforms for the corresponding template. In this example, there is only one group because we only use one template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_new_detections(filename, metadata, waveforms, db_path='./output/'):\n",
    "    filename_meta = db_path + filename + 'meta.h5'\n",
    "    filename_wave = db_path + filename + 'wav.h5'\n",
    "    n_templates = len(metadata)\n",
    "    with h5.File(filename_meta, mode='w') as f:\n",
    "        for t in range(n_templates):\n",
    "            if len(metadata[t]['origin_times']) == 0:\n",
    "                # no detection\n",
    "                continue\n",
    "            f.create_group('{:d}'.format(metadata[t]['template_id'][0]))\n",
    "            for key in metadata[t].keys():\n",
    "                f['{:d}'.format(metadata[t]['template_id'][0])].create_dataset(key, data=metadata[t][key], compression='gzip')\n",
    "    with h5.File(filename_wave, mode='w') as f:\n",
    "        for t in range(n_templates):\n",
    "            if len(metadata[t]['origin_times']) == 0:\n",
    "                # no detection\n",
    "                continue\n",
    "            f.create_group('{:d}'.format(metadata[t]['template_id'][0]))\n",
    "            f['{:d}'.format(metadata[t]['template_id'][0])].create_dataset('waveforms', data=waveforms[t]['waveforms'], compression='lzf')\n",
    "            print('{:d} events detected with Template {:d}'.format(waveforms[t]['waveforms'].shape[0], metadata[t]['template_id'][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data and the template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data from day 2013-03-17\n",
    "data = utils.load_data('data_FMF_tutorial.h5')\n",
    "\n",
    "# load the template event that we have just built\n",
    "template = utils.load_template('template.h5', path='./output/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format the moveouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moveouts = np.hstack( (template['moveouts_S'].reshape(-1, 1),\n",
    "                       template['moveouts_S'].reshape(-1, 1),\n",
    "                       template['moveouts_P'].reshape(-1, 1)) )\n",
    "moveout_array = moveouts[np.newaxis, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the correlation coefficient time series that we saved previsouly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_sum = utils.load_cc('cc_sum.h5', path='./output/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fix the detection threshold and extract the events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the events with cc n_mad times higher than MAD\n",
    "n_mad = 10.\n",
    "metadata, waveforms = extract_new_detections(data, cc_sum, moveout_array, n_mad=n_mad)\n",
    "print('metadata and waveforms are Python lists with lengths {:d} and {:d}\\\n",
    " (respectively) because we use a single template.\\n'.format(len(metadata), len(waveforms)))\n",
    "print('Elements of metadata are dictionaries with information on:\\n', list(metadata[0].keys()))\n",
    "print('\\n')\n",
    "print('Elements of waveforms are numpy arrays with shape:\\n', waveforms[0]['waveforms'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store the extracted events in an h5 database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_new_detections('detections_20130317_{:d}_mad'.format(int(n_mad)), metadata, waveforms, db_path='./output/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
